<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0">
  <title>Taaranator Final Project</title>

  <!-- Google Font -->
  <link href="https://fonts.googleapis.com/css2?family=Baloo+2:wght@400;700&display=swap" rel="stylesheet">
  <!-- Main stylesheet -->
  <link rel="stylesheet" href="styles.css">
</head>
<body>



  <!-- NAVBAR -->
  <nav>
    <ul>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#motivation">Motivation</a></li>
      <li><a href="#system-design">System Design</a></li>
      <li><a href="#software">Software</a></li>
      <li><a href="#hardware">Hardware</a></li>
      <li><a href="#sprint-progress">Sprint Progress</a></li>
      <li><a href="#final-demo">Final Demo</a></li>
      <li><a href="#video">Video</a></li>
      <li><a href="#images">Images</a></li>
      <li><a href="#results">Results</a></li>
      <li><a href="#conclusion">Conclusion</a></li>
      <li><a href="#references">References</a></li>
    </ul>
  </nav>

  <!-- HERO -->
  <header class="hero">
    <div class="truck">ðŸšš</div>
    <h1>Taaranator Final Project Report</h1>
    <img src="image\README\IMG_4348.JPG" alt="Taaranator Robot" class="hero-image">
  </header>

  <!-- MAIN CONTENT -->
  <main>
    <section id="abstract & motivation">
      <h2>Abstract & Motivation</h2>
      <p>We designed and built an autonomous robot that detects, collects, and disposes of lightweight trash using computer vision and embedded systems. Our robot scans its surroundings to identify yellow blocks as trash, moves toward the target, collects it with a custom shovel, and deposits it against a wall detected by an ultrasonic sensor. After each collection, it rotates and resumes scanning for new objects. The system combines sensor integration, actuator control, serial communication, and bare-metal C programming to create a modular and practical cleanup solution. Our project not only demonstrates key embedded and robotics principles but also opens the door for future improvements, such as real-time object recognition and advanced autonomous navigation for indoor and outdoor environments.</p>
    </section>

    <section id="video">
      <h2>Video</h2>
      <p>The live demo shows Taaranator identifying a small object, driving forward, engaging its shovel to scoop, and then reversing to place it at the beaconâ€” all autonomously, with wall avoidance.</p>
      <p><a href="https://youtu.be/YOUR_VIDEO_LINK" target="_blank">â–¶ Watch our 5-minute demo video</a></p>
    </section>


    <section id="images">
      <h2>Images</h2>
      <div class="gallery">
        <img src="image\README\WhatsApp Image 2025-04-28 at 12.36.11_44663a65.jpg" alt="Top view">
        <img src="image2.jpg" alt="Electronics layout">
        <img src="image\README\WhatsApp Image 2025-04-28 at 12.37.47_ce98078c.jpg" alt="Shovel">
      </div>
    </section>

    <section id="Software Requirements Specification Validation">
      <h2>SRS Validation</h2>
      <p>During development, we made some adjustments to our original software requirements to better align with the project goals and hardware constraints. While we initially planned to use an IR sensor for trash detection and I2C for communication, we successfully transitioned to using machine learning with computer vision for trash detection and SPI communication between the Raspberry Pi and ATMega328PB for faster, more reliable data transfer. We also replaced ultrasonic sensor polling with timer-based interrupts to improve distance detection responsiveness. The following summarizes our final SRS compliance and validation: </p>
      <ul>
        <li>SRS-01: Sensor Polling Frequency (Updated):
          Instead of polling, we implemented timer-based interrupts for the ultrasonic sensor, using timer 3. The system behavior confirmed that wall detection was responsive without lag.
        </li>
        <li>SRS-02: Trash Detection (Updated):
          We replaced the IR sensor with a YOLO machine learning model running on the Raspberry Pi. In testing, the robot reliably detected yellow blocks placed within view of the camera and triggered collection behavior appropriately.
        </li>
        <li>SRS-03: Obstacle Avoidance Threshold:
          Wall detection using the ultrasonic sensor remained as planned. We manually validated behavior by observing that the robot stopped moving when approaching walls within about 20â€“30 cm.
        </li>
        <li>SRS-04: Image Processing (Updated):
          The Raspberry Pi captured and processed images at approximately 1 frame per second. We observed that the robot responded to trash detection reliably.
        </li>
        <li>SRS-05: PWM Motor Control Timing:
          Motor PWM control was implemented with Timer0 at a target of ~500 Hz. We observed smooth and consistent motor operation without perceptible jitter.
        </li>
        <li>SRS-06: SPI Communication Protocol (Updated):
          Instead of I2C, we implemented SPI communication between the Raspberry Pi and ATMega328PB for transmitting navigation directions. We confirmed low-latency updates from detection to motor action.
        </li>
        <li>SRS-07: User Interrupt & Safety Response:
          An emergency stop button connected to PC0 was successfully integrated using a pin-change interrupt. During testing, the motors stopped within 100 ms of button press, as confirmed by manual tests.
        </li>
      </ul>

      <h3>proof</h3>
      <p><a href="https://drive.google.com/file/d/1o-LlPXBNlsq7b0NR9ot6adOTAn2zIAc2/view?usp=sharing" target="_blank">â–¶ Watch our motor control (forward, backwards, speed) video</a></p>
      <p><a href="https://drive.google.com/file/d/1erBPFCrLf8zJfBMNw_j0bJg6JB4yxkOM/view?usp=sharing" target="_blank">â–¶ Watch our motor rotation video</a></p>
      <p>SPI Dummy Value test</p>
      <img src="image\README\1744933511560.png" alt="SPI Dummy Value test" class="hero-image">
      <p><a href="https://drive.google.com/file/d/1n4UYuvp0TMSXZDLKCUhrddrH8jbGPYtZ/view?usp=sharing" target="_blank">â–¶ Watch our camera detecting the block video</a></p>
      <p><a href="https://drive.google.com/file/d/12GPv1BvtwwBkfn28_EqNGDv7AikHxlrk/view?usp=sharing" target="_blank">â–¶ Watch our camera perceiving distance of the block video</a></p>
      <p>Confusion Matrix Normalized - prequantized model</p>
      <img src="image\README\confusion matrix normalized.jpg" alt="Confusion Matrix Normalized- prequantized model" class="hero-image">
      <p>Composite of images - prequantized model</p>
      <img src="image\README\composite of images.jpg" alt="Composite of images-prequantized model" class="hero-image">
    
    </section>

    <section id="Hardware Requirements Specification Validation">
      <h2>HRS Validation</h2>
      <p>Throughout the development process, we adapted several of our hardware designs and configurations to better meet practical constraints and testing outcomes. For example, we replaced our nonfunctional logic level shifter with a voltage divider, substituted I2C with SPI for robustness, and used a power bank instead of a 6V battery with a buck converter. We also added a rear counterweight to balance the shovel, and verified long-term power stability over extended runs. The following summarizes our final HRS compliance and validation:</p>
      <ul>
        <li>HRS-01: Drive Motor Performance:
          The robot was able to drive forward while carrying a load. Although ramp testing was not conducted, the motors provided sufficient torque for smooth motion and start-up. A counterweight at the back successfully balanced the front shovel. We validated speed across 1 meter and observed no stalling or overheating.
        </li>
        <li>HRS-02: Trash Detection via Camera instead of IR Sensor (Updated):
        We did not use an IR sensor. Instead, we used a forward-facing Raspberry Pi camera paired with a YOLO machine learning model to detect trash objects. The camera reliably identified yellow blocks placed in the robotâ€™s path and triggered collection behavior accordingly. Detection performance was stable across multiple trials and lighting conditions.
        </li>
        <li>HRS-03: Ultrasonic Sensor Distance & Accuracy:
          We placed obstacles at 10 cm, 50 cm, 100 cm, 200 cm, and 300 cm and recorded 10 readings at each distance. Measurement error was within Â±1 cm for short range and Â±5 cm for long range. The sensor consistently detected objects up to 3 meters without false negatives.
        </li>
        <li>HRS-04: Camera Setup:
          The Arducam was mounted securely and delivered live image captures at â‰¥480p resolution and 1 FPS. The camera remained stable during movement, and frame delivery to the Pi was confirmed during trash detection testing.
        </li>
        <li>HRS-05: Battery and Power System:
          A power bank was used to supply the required motor and 5 V logic rails. Under high load, the 5 V line remained within 4.75â€“5.25 V, and no brown-outs or instability were observed. The robot ran continuously for at least 2 hours, exceeding the 1-hour requirement.
        </li>
        <li>HRS-06: Logic Level Interface (SPI Bus Hardware):
          SPI communication between the 5 V ATMega328PB and 3.3 V Raspberry Pi was implemented using a voltage divider. Oscilloscope readings showed clean signal transitions, and extended communication tests confirmed stability and accuracy without significant packet loss, even under motor noise.
        </li>
      </ul>
    </section>

    <h3>proof</h3>


    <section id="conclusion">
      <h2>Conclusion</h2>
      <p></p>
    </section>

  </main>

  <!-- FOOTER -->
  <footer>
    <p>Team CAT â€” Chekayli Meyer â€¢ Andrea GonzÃ¡lez Varela â€¢ Taarana Jammula</p>
    <p>&copy; 2025 ESE 3500, UPenn</p>
  </footer>

  <!-- CURSOR TRAIL SCRIPT -->
  <script>
    document.addEventListener('mousemove', e => {
      const trail = document.createElement('div');
      trail.className = 'trail';
      trail.textContent = 'ðŸ”¥';
      trail.style.left = e.clientX + 'px';
      trail.style.top  = e.clientY + 'px';
      document.body.appendChild(trail);
      setTimeout(() => trail.remove(), 700);
    });
  </script>
</body>
</html>
