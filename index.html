<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0">
  <title>Taaranator Final Project</title>

  <!-- Google Font -->
  <link href="https://fonts.googleapis.com/css2?family=Baloo+2:wght@400;700&display=swap" rel="stylesheet">
  <!-- Main stylesheet -->
  <link rel="stylesheet" href="styles.css">
</head>
<body>



  <!-- NAVBAR -->
  <nav>
    <ul>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#motivation">Motivation</a></li>
      <li><a href="#system-design">System Design</a></li>
      <li><a href="#software">Software</a></li>
      <li><a href="#hardware">Hardware</a></li>
      <li><a href="#sprint-progress">Sprint Progress</a></li>
      <li><a href="#final-demo">Final Demo</a></li>
      <li><a href="#video">Video</a></li>
      <li><a href="#images">Images</a></li>
      <li><a href="#results">Results</a></li>
      <li><a href="#conclusion">Conclusion</a></li>
      <li><a href="#references">References</a></li>
    </ul>
  </nav>

  <!-- HERO -->
  <header class="hero">
    <div class="truck">üöö</div>
    <h1>Taaranator Final Project Report</h1>
    <img src="image\README\IMG_4348.JPG" alt="Taaranator Robot" class="hero-image">
  </header>

  <!-- MAIN CONTENT -->
  <main>
    <section id="abstract & motivation">
      <h2>Abstract & Motivation</h2>
      <p>We designed and built an autonomous robot that detects, collects, and disposes of lightweight trash using computer vision and embedded systems. Our robot scans its surroundings to identify yellow blocks as trash, moves toward the target, collects it with a custom shovel, and deposits it against a wall detected by an ultrasonic sensor. After each collection, it rotates and resumes scanning for new objects. The system combines sensor integration, actuator control, serial communication, and bare-metal C programming to create a modular and practical cleanup solution. Our project not only demonstrates key embedded and robotics principles but also opens the door for future improvements, such as real-time object recognition and advanced autonomous navigation for indoor and outdoor environments.</p>
    </section>

    <section id="system-design">
      <h2>System Design</h2>
      <p>The block diagram features a 6 V AA battery pack ‚Üí buck converter ‚Üí ATMega328PB for motor & sensor control, plus a Raspberry Pi with PiCam V3 for vision. Communication runs over SPI with level‚Äêshifting. A laser‚Äêcut shovel plate on the front captures debris.</p>
    </section>

    <section id="software">
      <h2>Software</h2>
      <ul>
        <li>Sensor Polling @ ‚â•10 Hz (IR + Ultrasonic)</li>
        <li>Trash & Obstacle Threshold Detection</li>
        <li>PWM Motor Control (‚â•100 Hz)</li>
        <li>SPI Protocol w/ ACK/NACK</li>
        <li>YOLO-based Object Detection on Pi</li>
        <li>Emergency Stop GPIO Interrupt</li>
      </ul>
    </section>

    <section id="hardware">
      <h2>Hardware</h2>
      <ul>
        <li>ATMega328PB & L298N Driver</li>
        <li>Raspberry Pi + PiCam V3</li>
        <li>HC-SR04P Ultrasonic & IR Proximity Sensors</li>
        <li>12√óAA Battery Pack + Buck Converter</li>
        <li>Bi-directional Logic Level Shifter</li>
        <li>Laser-cut Acrylic Chassis & 3D-printed Shovel</li>
      </ul>
    </section>

    <section id="sprint-progress">
      <h2>Sprint Progress</h2>
      <ol>
        <li>Assembled chassis, wired power & sensors</li>
        <li>Implemented motor & ultrasonic drivers on ATMega</li>
        <li>Built SPI link & Python client on Pi</li>
        <li>Trained preliminary YOLO model for trash detection</li>
        <li>Integrated full loop: detect ‚Üí approach ‚Üí scoop ‚Üí deliver</li>
      </ol>
    </section>

    <section id="final-demo">
      <h2>Final Demo</h2>
      <p>The live demo shows Taaranator identifying a small object, driving forward, engaging its shovel to scoop, and then reversing to place it at the beacon‚Äî all autonomously, with wall avoidance.</p>
    </section>

    <section id="video">
      <h2>Video</h2>
      <p><a href="https://youtu.be/YOUR_VIDEO_LINK" target="_blank">‚ñ∂ Watch our 5-minute demo video</a></p>
    </section>

    <section id="images">
      <h2>Images</h2>
      <div class="gallery">
        <img src="image\README\WhatsApp Image 2025-04-28 at 12.36.11_44663a65.jpg" alt="Top view">
        <img src="image2.jpg" alt="Electronics layout">
        <img src="image\README\WhatsApp Image 2025-04-28 at 12.37.47_ce98078c.jpg" alt="Shovel">
      </div>
    </section>

    <section id="results">
      <h2>Results</h2>
      <p>We met SRS-01 (‚â•10 Hz polling), SRS-03 (30 cm avoidance), and HRS-02 (‚â•15 cm IR detection @ 90% reliability). Proof lives in <code>/validation</code>.</p>
    </section>

    <section id="conclusion">
      <h2>Conclusion</h2>
      <p>Taaranator demonstrates a fully integrated embedded-vision mobile robot. We learned deep insights into SPI timing, real-time threading on Pi, and mechanical tolerancing for laser-cut parts. Next: refine our detection model and add SLAM navigation.</p>
    </section>

    <section id="references">
      <h2>References</h2>
      <ul>
        <li><a href="https://docs.github.com/en/pages/quickstart" target="_blank">GitHub Pages Quickstart</a></li>
        <li>HC-SR04P Ultrasonic Datasheet</li>
        <li>ONNX Runtime Quantization Guide</li>
        <li>Ultralytics YOLOv5 Docs</li>
      </ul>
    </section>
  </main>

  <!-- FOOTER -->
  <footer>
    <p>Team CAT ‚Äî Chekayli Meyer ‚Ä¢ Andrea Gonz√°lez Varela ‚Ä¢ Taarana Jammula</p>
    <p>&copy; 2025 ESE 3500, UPenn</p>
  </footer>

  <!-- CURSOR TRAIL SCRIPT -->
  <script>
    document.addEventListener('mousemove', e => {
      const trail = document.createElement('div');
      trail.className = 'trail';
      trail.textContent = 'üî•';
      trail.style.left = e.clientX + 'px';
      trail.style.top  = e.clientY + 'px';
      document.body.appendChild(trail);
      setTimeout(() => trail.remove(), 700);
    });
  </script>
</body>
</html>
