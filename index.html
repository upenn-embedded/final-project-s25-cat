<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0">
  <title>Taaranator Final Project</title>

  <!-- Google Font -->
  <link href="https://fonts.googleapis.com/css2?family=Baloo+2:wght@400;700&display=swap" rel="stylesheet">
  <!-- Main stylesheet -->
  <link rel="stylesheet" href="styles.css">
</head>
<body>



  <!-- NAVBAR -->
  <nav>
    <ul>
      <li><a href="#abstract">Abstract & Motivation</a></li>
      <li><a href="#video">Video</a></li>
      <li><a href="#images">Images</a></li>
      <li><a href="#srs-val">SRS Validation</a></li>
      <li><a href="#hrs-val">HRS Validation</a></li>
      <li><a href="#conclusion">Conclusion</a></li>
      <li><a href="#references">References</a></li>
    </ul>
  </nav>

  <!-- HERO -->
  <header class="hero">
    <div class="truck">ðŸšš</div>
    <h1>Taaranator Final Project Report</h1>
    <img src="image\README\IMG_4348.JPG" alt="Taaranator Robot" class="hero-image">
  </header>

  <!-- MAIN CONTENT -->
  <main>
    <section id="abstract">
      <h2>Abstract & Motivation</h2>
      <p>We designed and built an autonomous robot that detects, collects, and disposes of lightweight trash using computer vision and embedded systems. Our robot scans its surroundings to identify yellow blocks as trash, moves toward the target, collects it with a custom shovel, and deposits it against a wall detected by an ultrasonic sensor. After each collection, it rotates and resumes scanning for new objects. The system combines sensor integration, actuator control, serial communication, and bare-metal C programming to create a modular and practical cleanup solution. Our project not only demonstrates key embedded and robotics principles but also opens the door for future improvements, such as real-time object recognition and advanced autonomous navigation for indoor and outdoor environments.</p>
    </section>

    <section id="video">
      <h2>Video</h2>
      <p>The live demo shows Taaranator identifying a small object, driving forward, engaging its shovel to scoop, and then reversing to place it at the beaconâ€” all autonomously, with wall avoidance.</p>
      <p><a href="https://drive.google.com/file/d/1tgKRL_t79QS1e0g7a95CM60AKaz--BF2/view?usp=sharing" target="_blank">â–¶ Watch our 5-minute demo video</a></p>
    </section>


    <section id="images">
      <h2>Images</h2>
      <div class="gallery">
        <img src="image\README\WhatsApp Image 2025-04-28 at 12.36.11_44663a65.jpg" alt="Top view">
        <img src="image\README\Topview.png" alt="Electronics layout">
        <img src="image\README\WhatsApp Image 2025-04-28 at 12.37.47_ce98078c.jpg" alt="Shovel">
      </div>
    </section>

    <section id="srs-val">
      <h2>SRS Validation</h2>
      <p>During development, we made some adjustments to our original software requirements to better align with the project goals and hardware constraints. While we initially planned to use an IR sensor for trash detection and I2C for communication, we successfully transitioned to using machine learning with computer vision for trash detection and SPI communication between the Raspberry Pi and ATMega328PB for faster, more reliable data transfer. We also replaced ultrasonic sensor polling with timer-based interrupts to improve distance detection responsiveness. The following summarizes our final SRS compliance and validation: </p>
      <ul>
        <li>SRS-01: Sensor Polling Frequency (Updated):
          Instead of polling, we implemented timer-based interrupts for the ultrasonic sensor, using timer 3. The system behavior confirmed that wall detection was responsive without lag.
        </li>
        <li>SRS-02: Trash Detection (Updated):
          We replaced the IR sensor with a YOLO machine learning model running on the Raspberry Pi. In testing, the robot reliably detected yellow blocks placed within view of the camera and triggered collection behavior appropriately.
        </li>
        <li>SRS-03: Obstacle Avoidance Threshold:
          Wall detection using the ultrasonic sensor remained as planned. We manually validated behavior by observing that the robot stopped moving when approaching walls within about 20â€“30 cm.
        </li>
        <li>SRS-04: Image Processing (Updated):
          The Raspberry Pi captured and processed images at approximately 1 frame per second. We observed that the robot responded to trash detection reliably.
        </li>
        <li>SRS-05: PWM Motor Control Timing:
          Motor PWM control was implemented with Timer0 at a target of ~500 Hz. We observed smooth and consistent motor operation without perceptible jitter.
        </li>
        <li>SRS-06: SPI Communication Protocol (Updated):
          Instead of I2C, we implemented SPI communication between the Raspberry Pi and ATMega328PB for transmitting navigation directions. We confirmed low-latency updates from detection to motor action.
        </li>
        <li>SRS-07: User Interrupt & Safety Response:
          An emergency stop button connected to PC0 was successfully integrated using a pin-change interrupt. During testing, the motors stopped within 100 ms of button press, as confirmed by manual tests.
        </li>
      </ul>

      <h3>proof</h3>
      <p><a href="https://drive.google.com/file/d/1o-LlPXBNlsq7b0NR9ot6adOTAn2zIAc2/view?usp=sharing" target="_blank">â–¶ Watch our motor control (forward, backwards, speed) video</a></p>
      <p><a href="https://drive.google.com/file/d/1erBPFCrLf8zJfBMNw_j0bJg6JB4yxkOM/view?usp=sharing" target="_blank">â–¶ Watch our motor rotation video</a></p>
      <p>SPI Dummy Value test</p>
      <img src="image\README\1744933511560.png" alt="SPI Dummy Value test" class="hero-image">
      <p><a href="https://drive.google.com/file/d/1n4UYuvp0TMSXZDLKCUhrddrH8jbGPYtZ/view?usp=sharing" target="_blank">â–¶ Watch our camera detecting the block video</a></p>
      <p><a href="https://drive.google.com/file/d/12GPv1BvtwwBkfn28_EqNGDv7AikHxlrk/view?usp=sharing" target="_blank">â–¶ Watch our camera perceiving distance of the block video</a></p>
      <p>Confusion Matrix Normalized - prequantized model</p>
      <img src="image\README\confusion matrix normalized.jpg" alt="Confusion Matrix Normalized- prequantized model" class="hero-image">
      <p>Composite of images - prequantized model</p>
      <img src="image\README\composite of images.jpg" alt="Composite of images-prequantized model" class="hero-image">
    
    </section>

    <section id="hrs-val">
      <h2>HRS Validation</h2>
      <p>Throughout the development process, we adapted several of our hardware designs and configurations to better meet practical constraints and testing outcomes. For example, we replaced our nonfunctional logic level shifter with a voltage divider, substituted I2C with SPI for robustness, and used a power bank instead of a 6V battery with a buck converter. We also added a rear counterweight to balance the shovel, and verified long-term power stability over extended runs. The following summarizes our final HRS compliance and validation:</p>
      <ul>
        <li>HRS-01: Drive Motor Performance:
          The robot was able to drive forward while carrying a load. Although ramp testing was not conducted, the motors provided sufficient torque for smooth motion and start-up. A counterweight at the back successfully balanced the front shovel. We validated speed across 1 meter and observed no stalling or overheating.
        </li>
        <li>HRS-02: Trash Detection via Camera instead of IR Sensor (Updated):
        We did not use an IR sensor. Instead, we used a forward-facing Raspberry Pi camera paired with a YOLO machine learning model to detect trash objects. The camera reliably identified yellow blocks placed in the robotâ€™s path and triggered collection behavior accordingly. Detection performance was stable across multiple trials and lighting conditions.
        </li>
        <li>HRS-03: Ultrasonic Sensor Distance & Accuracy:
          We placed obstacles at 10 cm, 50 cm, 100 cm, 200 cm, and 300 cm and recorded 10 readings at each distance. Measurement error was within Â±1 cm for short range and Â±5 cm for long range. The sensor consistently detected objects up to 3 meters without false negatives.
        </li>
        <li>HRS-04: Camera Setup:
          The Arducam was mounted securely and delivered live image captures at â‰¥480p resolution and 1 FPS. The camera remained stable during movement, and frame delivery to the Pi was confirmed during trash detection testing.
        </li>
        <li>HRS-05: Battery and Power System:
          A power bank was used to supply the required motor and 5 V logic rails. Under high load, the 5 V line remained within 4.75â€“5.25 V, and no brown-outs or instability were observed. The robot ran continuously for at least 2 hours, exceeding the 1-hour requirement.
        </li>
        <li>HRS-06: Logic Level Interface (SPI Bus Hardware):
          SPI communication between the 5 V ATMega328PB and 3.3 V Raspberry Pi was implemented using a voltage divider. Oscilloscope readings showed clean signal transitions, and extended communication tests confirmed stability and accuracy without significant packet loss, even under motor noise.
        </li>
      </ul>
    </section>

    <h3>proof</h3>
      <p><a href="https://drive.google.com/file/d/17VDj15f5mGnT_kkAiv49TOgbWERdB4tv/view?usp=sharing" target="_blank">â–¶ Watch our motor stop with ultrasonic wall detection video</a></p>
      <p>Ultrasonic detecting distances</p>
      <img src="image\README\1744937258654.png" alt="Ultrasonic detecting distances" class="hero-image">
      <p>Raspberry Pi</p>
      <img src="image\README\rpi.png" alt="Raspberry Pi" class="hero-image">
      <p>Camera & Ultrasonic Sensor</p>
      <img src="image\README\Camera & Ultrasonic.png" alt="Camera & Ultrasonic Sensor" class="hero-image">
      <p>Motor Driver & Power Bank</p>
      <img src="image\README\motor driver and power bank.png" alt="Motor Driver & Power Bank" class="hero-image">
      <p>Top view: ATMega, Raspberry Pi, Emergency Stop Push button & wiring</p>
      <img src="image\README\Topview.png" alt="Top view: ATMega, Raspberry Pi, Emergency Stop Push button & wiring" class="hero-image">
      <p>ATMega wiring</p>
      <img src="image\README\ATMega Wiring.png" alt="ATMega wiring" class="hero-image">
      <p>Raspberry Pi wiring</p>
      <img src="image\README\Raspberry Pi Wiring.png" alt="Raspberry Pi wiring" class="hero-image">

    <section id="conclusion">
      <h2>Conclusion</h2>
        <p>Throughout the project, we learned a lot about integrating different hardware and software components into a cohesive system. We adapted our design as challenges arose and found creative solutions to hardware and communication issues. Below is a summary of our project reflection:</p>
        <ul>
          <li><strong>What did we learn from it?</strong>
            We learned how to integrate subsystems such as motor control, SPI communication, ultrasonic sensing, and computer vision into a single working platform. We also gained experience troubleshooting hardware-level issues like voltage mismatches and adapting quickly when components did not perform as expected.
          </li>
          <li><strong>What went well?</strong>
            Motor control, SPI communication, and extended robot operation went very smoothly after setup. Our robot consistently detected objects using the camera and moved appropriately. We were able to successfully run the robot for long periods without brown-outs or crashes.
          </li>
          <li><strong>What accomplishments are we proud of?</strong>
            We are proud of successfully transitioning from using an IR sensor to a camera-based machine learning model for trash detection. We are also proud of implementing SPI communication using a voltage divider when our logic level shifter failed, and ensuring the robot was mechanically stable and robust during long-term testing.
          </li>
          <li><strong>What did we learn/gain from this experience?</strong>
            We gained valuable hands-on experience integrating hardware and software, troubleshooting electrical issues, and building systems that are reliable over time. We also developed stronger problem-solving skills by adjusting our approach when real-world conditions differed from expectations.
          </li>
          <li><strong>Did we have to change our approach?</strong>
            Yes. We changed from using an IR sensor to a camera for trash detection, switched from I2C to SPI communication for robustness, and replaced the 6V battery and buck converter setup with a simpler power bank solution. We also added a counterweight to balance the front-heavy design caused by the shovel.
          </li>
          <li><strong>What could have been done differently?</strong>
            We could have tested critical components like the level shifter and power system independently before integration. We also could have fine-tuned the software for ultrasonic distance measurement to improve accuracy and consistency. Adding additional ultrasonic sensors positioned at angles would have improved wall detection when approaching at non-head-on angles.
          </li>
          <li><strong>Did we encounter obstacles that we didnâ€™t anticipate?</strong>
            Yes. We did not anticipate the failure of the logic level shifter, which required building a voltage divider ourselves. We also did not initially plan for the shovelâ€™s front-heavy impact on the robotâ€™s balance, which required a counterweight solution.
          </li>
          <li><strong>What could be a next step for this project?</strong>
            Next steps could include improving the ultrasonic sensor software to filter noise and calibrate distances more precisely, as well as adding additional ultrasonic sensors at angles to improve obstacle detection and wall following when approaching walls from different directions. Lastly, we would train our supervised machine learning model on a large dataset of images of trash (which our yellow blocks served as a proxy for).
          </li>
        </ul>
    </section>

    <section id="References">
      <h2>References</h2>
        <ul>
          <li><a href="https://ww1.microchip.com/downloads/aemDocuments/documents/MCU08/ProductDocuments/DataSheets/40001906C.pdf">ATMega 328PB datasheet</a></li>
          <li><a href="https://github.com/upenn-embedded/final-project-s25-cat/blob/main/datasheets_and_models/ATmega328PB_Xplained_Mini_Schematics.pdf">ATmega328PB Xplained Mini Schematics</a></li>
          <li><a href="https://github.com/upenn-embedded/final-project-s25-cat/blob/b537aac7de03d78fe197f5bd12d081c1ee69fa79/datasheets_and_models/Romi%20Mainboard.PDF">KatzBot Datasheets</a></li>
          <li><a href="https://github.com/upenn-embedded/final-project-s25-cat/blob/8c9b48332c07fcebf3f4ddfdd915e39affeacde0/datasheets_and_models/hc-sr04-ultrasonic-sensor-8.snapshot.15.zip">Ultrasonic Sensor Documentation</a></li>
          <li><a href="https://datasheets.raspberrypi.com/rpi4/raspberry-pi-4-datasheet.pdf">Raspberry Pi 4 Model B</a></li>
          <li><a href="https://www.raspberrypi.com/software/">Raspberry Pi OS</a></li>
          <li><a href="https://github.com/ultralytics/yolov5">Ultralytics YOLOv5 documentation</a></li>
          <li><a href="https://pypi.org/project/opencv-python/">OpenCV Python documentation</a></li>
          <li><a href="https://onnxruntime.ai/">ONNX Runtime documentation</a></li>
          <li><a href="https://pypi.org/project/spidev/">Python Spidev documentation</a></li>
          <li><a href="https://numpy.org/">Numpy documentation</a></li>
        </ul>
    </section>


  </main>

  <!-- FOOTER -->
  <footer>
    <p>Team CAT â€” Chekayli Meyer â€¢ Andrea GonzÃ¡lez Varela â€¢ Taarana Jammula</p>
    <p>&copy; 2025 ESE 3500, UPenn</p>
  </footer>

  <!-- CURSOR TRAIL SCRIPT -->
  <script>
    document.addEventListener('mousemove', e => {
      const trail = document.createElement('div');
      trail.className = 'trail';
      trail.textContent = 'ðŸ”¥';
      trail.style.left = e.clientX + 'px';
      trail.style.top  = e.clientY + 'px';
      document.body.appendChild(trail);
      setTimeout(() => trail.remove(), 700);
    });
  </script>
</body>
</html>
