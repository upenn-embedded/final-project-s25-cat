<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0">
  <title>Taaranator Final Project</title>

  <!-- Google Font -->
  <link href="https://fonts.googleapis.com/css2?family=Baloo+2:wght@400;700&display=swap" rel="stylesheet">
  <!-- Main stylesheet -->
  <link rel="stylesheet" href="styles.css">
</head>
<body>



  <!-- NAVBAR -->
  <nav>
    <ul>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#motivation">Motivation</a></li>
      <li><a href="#system-design">System Design</a></li>
      <li><a href="#software">Software</a></li>
      <li><a href="#hardware">Hardware</a></li>
      <li><a href="#sprint-progress">Sprint Progress</a></li>
      <li><a href="#final-demo">Final Demo</a></li>
      <li><a href="#video">Video</a></li>
      <li><a href="#images">Images</a></li>
      <li><a href="#results">Results</a></li>
      <li><a href="#conclusion">Conclusion</a></li>
      <li><a href="#references">References</a></li>
    </ul>
  </nav>

  <!-- HERO -->
  <header class="hero">
    <div class="truck">ðŸšš</div>
    <h1>Taaranator Final Project Report</h1>
    <img src="image\README\IMG_4348.JPG" alt="Taaranator Robot" class="hero-image">
  </header>

  <!-- MAIN CONTENT -->
  <main>
    <section id="abstract & motivation">
      <h2>Abstract & Motivation</h2>
      <p>We designed and built an autonomous robot that detects, collects, and disposes of lightweight trash using computer vision and embedded systems. Our robot scans its surroundings to identify yellow blocks as trash, moves toward the target, collects it with a custom shovel, and deposits it against a wall detected by an ultrasonic sensor. After each collection, it rotates and resumes scanning for new objects. The system combines sensor integration, actuator control, serial communication, and bare-metal C programming to create a modular and practical cleanup solution. Our project not only demonstrates key embedded and robotics principles but also opens the door for future improvements, such as real-time object recognition and advanced autonomous navigation for indoor and outdoor environments.</p>
    </section>

    <section id="Software Requirements Specification Validation">
      <h2>SRS Validation</h2>
      <p>During development, we made several adjustments to our original software requirements to better align with the project goals and hardware constraints. While we initially planned to use an IR sensor for trash detection and I2C for communication, we successfully transitioned to using machine learning with computer vision for trash detection and SPI communication between the Raspberry Pi and ATMega328PB for faster, more reliable data transfer. We also replaced ultrasonic sensor polling with timer-based interrupts to improve distance detection responsiveness. The following summarizes our final SRS compliance and validation: </p>
      <ul>
        <li>SRS-01: Sensor Polling Frequency (Updated):
          Instead of polling, we implemented timer-based interrupts for the ultrasonic sensor, using timer 3. 
        </li>
        <li>SRS-02: Trash Detection (Updated):
          We replaced the IR sensor with a machine learning model (YOLO) running on the Raspberry Pi, enabling accurate detection of trash objects based on visual input. We verified this by placing yellow blocks in view of the camera and confirming that image capture, inference, and SPI command transmission occurred reliably.
        </li>
        <li>SRS-03: Obstacle Avoidance Threshold:
          Wall detection with the ultrasonic sensor remained as planned. We validated that when an object was detected within 20â€“30 cm, the robot stopped moving within 100 ms, as confirmed by high-speed video captures of motor control signals.
        </li>
        <li>SRS-04: Image Processing (Updated):
          The Raspberry Pi captured and processed images at approximately 1 frame per second or faster. We verified through log timestamps that detection-to-command delay remained under 0.5 seconds, satisfying the timing requirement even with the added YOLO model processing overhead.
        </li>
        <li>SRS-05: PWM Motor Control Timing:
          Motor PWM control was implemented with Timer0 on the ATMega328PB.
        </li>
        <li>SRS-06: SPI Communication Protocol (Updated):
          Instead of I2C, we implemented SPI communication between the Raspberry Pi and ATMega328PB for transmitting navigation directions. We verified correct data transfers using SPI logic analyzer captures and confirmed low-latency updates from detection to motor action.
        </li>
        <li>SRS-07: User Interrupt & Safety Response:
          An emergency stop button connected to PC0 was successfully integrated using a pin-change interrupt. During testing, the motors stopped within 100 ms of button press, as confirmed by oscilloscope measurements and manual tests.
        </li>
      </ul>
    </section>

    <section id="Hardware Requirements Specification Validation">
      <h2>HRS Validation</h2>
      <ul>
        <li>ATMega328PB & L298N Driver</li>
        <li>Raspberry Pi + PiCam V3</li>
        <li>HC-SR04P Ultrasonic & IR Proximity Sensors</li>
        <li>12Ã—AA Battery Pack + Buck Converter</li>
        <li>Bi-directional Logic Level Shifter</li>
        <li>Laser-cut Acrylic Chassis & 3D-printed Shovel</li>
      </ul>
    </section>

    <section id="sprint-progress">
      <h2>Sprint Progress</h2>
      <ol>
        <li>Assembled chassis, wired power & sensors</li>
        <li>Implemented motor & ultrasonic drivers on ATMega</li>
        <li>Built SPI link & Python client on Pi</li>
        <li>Trained preliminary YOLO model for trash detection</li>
        <li>Integrated full loop: detect â†’ approach â†’ scoop â†’ deliver</li>
      </ol>
    </section>

    <section id="final-demo">
      <h2>Final Demo</h2>
      <p>The live demo shows Taaranator identifying a small object, driving forward, engaging its shovel to scoop, and then reversing to place it at the beaconâ€” all autonomously, with wall avoidance.</p>
    </section>

    <section id="video">
      <h2>Video</h2>
      <p><a href="https://youtu.be/YOUR_VIDEO_LINK" target="_blank">â–¶ Watch our 5-minute demo video</a></p>
    </section>

    <section id="images">
      <h2>Images</h2>
      <div class="gallery">
        <img src="image\README\WhatsApp Image 2025-04-28 at 12.36.11_44663a65.jpg" alt="Top view">
        <img src="image2.jpg" alt="Electronics layout">
        <img src="image\README\WhatsApp Image 2025-04-28 at 12.37.47_ce98078c.jpg" alt="Shovel">
      </div>
    </section>

    <section id="results">
      <h2>Results</h2>
      <p>We met SRS-01 (â‰¥10 Hz polling), SRS-03 (30 cm avoidance), and HRS-02 (â‰¥15 cm IR detection @ 90% reliability). Proof lives in <code>/validation</code>.</p>
    </section>

    <section id="conclusion">
      <h2>Conclusion</h2>
      <p>Taaranator demonstrates a fully integrated embedded-vision mobile robot. We learned deep insights into SPI timing, real-time threading on Pi, and mechanical tolerancing for laser-cut parts. Next: refine our detection model and add SLAM navigation.</p>
    </section>

    <section id="references">
      <h2>References</h2>
      <ul>
        <li><a href="https://docs.github.com/en/pages/quickstart" target="_blank">GitHub Pages Quickstart</a></li>
        <li>HC-SR04P Ultrasonic Datasheet</li>
        <li>ONNX Runtime Quantization Guide</li>
        <li>Ultralytics YOLOv5 Docs</li>
      </ul>
    </section>
  </main>

  <!-- FOOTER -->
  <footer>
    <p>Team CAT â€” Chekayli Meyer â€¢ Andrea GonzÃ¡lez Varela â€¢ Taarana Jammula</p>
    <p>&copy; 2025 ESE 3500, UPenn</p>
  </footer>

  <!-- CURSOR TRAIL SCRIPT -->
  <script>
    document.addEventListener('mousemove', e => {
      const trail = document.createElement('div');
      trail.className = 'trail';
      trail.textContent = 'ðŸ”¥';
      trail.style.left = e.clientX + 'px';
      trail.style.top  = e.clientY + 'px';
      document.body.appendChild(trail);
      setTimeout(() => trail.remove(), 700);
    });
  </script>
</body>
</html>
